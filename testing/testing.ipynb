{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "sys.path.append(os.getenv('ROOT_DIR'))\n",
    "\n",
    "RAW_DATA_DIR = os.getenv('RAW_DATA_DIR')\n",
    "PROCESSED_DATA_DIR = os.getenv('PROCESSED_DATA_DIR')\n",
    "TESTING_DATA_DIR = os.getenv('TESTING_DATA_DIR')\n",
    "PROCESSED_TESTING_CONTEXTS_PATH = os.path.join(PROCESSED_DATA_DIR,\"contexts-test.jsonl\")\n",
    "PROCESSED_TESTING_QUERIES_PATH = os.path.join(PROCESSED_DATA_DIR,\"queries-test.jsonl\")\n",
    "DATA_LANG_DICT = {\"squad\":\"en\", \"korquad\":\"ko\", \"fquad\":\"fr\", \"germanquad\":\"de\", \"uitviquad\":\"vi\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Starting delete existing files in processed folder\n",
      "\n",
      ">> Starting delete existing files in testing input folder\n",
      "\n",
      "---Created contexts file at /Volumes/Users/ly_k1/Documents/mColBERT/data/processed/contexts-test.jsonl\n",
      "---Created queries file at /Volumes/Users/ly_k1/Documents/mColBERT/data/processed/queries-test.jsonl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def create_context_and_query_files(contexts_filename = PROCESSED_TESTING_CONTEXTS_PATH, queries_filename = PROCESSED_TESTING_QUERIES_PATH):\n",
    "\n",
    "    with open(contexts_filename, \"w\") as f:\n",
    "        # Create an empty JSONL file\n",
    "        f.write(\"\")\n",
    "    print(\"---Created contexts file at {filename}\".format(filename=contexts_filename))\n",
    "\n",
    "    with open(queries_filename, \"w\") as f:\n",
    "        # Create an empty JSONL file\n",
    "        f.write(\"\")\n",
    "    \n",
    "    print(\"---Created queries file at {filename}\".format(filename=queries_filename))\n",
    "\n",
    "def main():\n",
    "\n",
    "    #create data directories or delete previous testing files\n",
    "    if not os.path.exists(PROCESSED_DATA_DIR):\n",
    "        os.system(\"mkdir {dir}\".format(dir=PROCESSED_DATA_DIR))\n",
    "    else:\n",
    "        print(\">> Starting delete existing files in processed folder\")\n",
    "        print()\n",
    "        for filename in os.listdir(PROCESSED_DATA_DIR):\n",
    "            if filename.endswith(\"-test-translated.jsonl\") or filename.endswith(\"-test-neg.json\") or filename.endswith(\"-test.json\"):\n",
    "                os.system(\"rm -f {file}\".format(file = os.path.join(PROCESSED_DATA_DIR, filename)))\n",
    "\n",
    "    if not os.path.exists(TESTING_DATA_DIR):\n",
    "        os.system(\"mkdir {dir}\".format(dir=TESTING_DATA_DIR))\n",
    "\n",
    "    else:\n",
    "        print(\">> Starting delete existing files in testing input folder\")\n",
    "        print()\n",
    "        for filename in os.listdir(TESTING_DATA_DIR):\n",
    "            os.system(\"rm -f {file}\".format(file = os.path.join(TESTING_DATA_DIR, filename)))\n",
    "\n",
    "    #check for context and query file, create if not exist\n",
    "    create_context_and_query_files()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to Tabular form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed conversion of /Volumes/Users/ly_k1/Documents/mColBERT/data/raw/squad-test.json\n",
      "Saved to /Volumes/Users/ly_k1/Documents/mColBERT/data/processed/squad-test.json\n",
      "Added new contexts and queries to /Volumes/Users/ly_k1/Documents/mColBERT/data/processed/contexts-test.jsonl and /Volumes/Users/ly_k1/Documents/mColBERT/data/processed/queries-test.jsonl\n",
      "Completed conversion of /Volumes/Users/ly_k1/Documents/mColBERT/data/raw/korquad-test.json\n",
      "Saved to /Volumes/Users/ly_k1/Documents/mColBERT/data/processed/korquad-test.json\n",
      "Added new contexts and queries to /Volumes/Users/ly_k1/Documents/mColBERT/data/processed/contexts-test.jsonl and /Volumes/Users/ly_k1/Documents/mColBERT/data/processed/queries-test.jsonl\n",
      "Completed conversion of /Volumes/Users/ly_k1/Documents/mColBERT/data/raw/fquad-test.json\n",
      "Saved to /Volumes/Users/ly_k1/Documents/mColBERT/data/processed/fquad-test.json\n",
      "Added new contexts and queries to /Volumes/Users/ly_k1/Documents/mColBERT/data/processed/contexts-test.jsonl and /Volumes/Users/ly_k1/Documents/mColBERT/data/processed/queries-test.jsonl\n",
      "Completed conversion of /Volumes/Users/ly_k1/Documents/mColBERT/data/raw/germanquad-test.json\n",
      "Saved to /Volumes/Users/ly_k1/Documents/mColBERT/data/processed/germanquad-test.json\n",
      "Added new contexts and queries to /Volumes/Users/ly_k1/Documents/mColBERT/data/processed/contexts-test.jsonl and /Volumes/Users/ly_k1/Documents/mColBERT/data/processed/queries-test.jsonl\n",
      "Completed conversion of /Volumes/Users/ly_k1/Documents/mColBERT/data/raw/uitviquad-test.json\n",
      "Saved to /Volumes/Users/ly_k1/Documents/mColBERT/data/processed/uitviquad-test.json\n",
      "Added new contexts and queries to /Volumes/Users/ly_k1/Documents/mColBERT/data/processed/contexts-test.jsonl and /Volumes/Users/ly_k1/Documents/mColBERT/data/processed/queries-test.jsonl\n"
     ]
    }
   ],
   "source": [
    "from services.convert_data import convert_quad\n",
    "\n",
    "def main():\n",
    "    for data_name in DATA_LANG_DICT.keys():\n",
    "        RAW_DATA_PATH = os.path.join(RAW_DATA_DIR, \"{data}-test.json\".format(data=data_name))\n",
    "        TABULAR_DATA_PATH =  os.path.join(PROCESSED_DATA_DIR, \"{data}-test.json\".format(data=data_name))\n",
    "        convert_quad(RAW_DATA_PATH, TABULAR_DATA_PATH, PROCESSED_TESTING_CONTEXTS_PATH, PROCESSED_TESTING_QUERIES_PATH)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from services.translate_queries import translate_all\n",
    "            \n",
    "def main():\n",
    "    for data_name, data_lang in DATA_LANG_DICT.items():\n",
    "        #translate all queries from one language to all other languages and save to new files\n",
    "        TABULAR_DATA_PATH = os.path.join(PROCESSED_DATA_DIR, \"{data}-test.json\".format(data=data_name))\n",
    "        TRANSLATED_DATA_PATH = os.path.join(PROCESSED_DATA_DIR, \"{data}-test-translated.jsonl\".format(data=data_name))\n",
    "        TRANSLATED_LANGUAGES = [lang for lang in [\"en\", \"de\", \"ko\", \"fr\", \"vi\"] if lang != data_lang]\n",
    "\n",
    "        translate_all(TABULAR_DATA_PATH, data_lang, TRANSLATED_DATA_PATH, TRANSLATED_LANGUAGES, PROCESSED_TESTING_QUERIES_PATH, PROCESSED_TESTING_QUERIES_PATH)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted /Volumes/Users/ly_k1/Documents/mColBERT/data/processed/contexts-test.jsonl to /Volumes/Users/ly_k1/Documents/mColBERT/data/testing/contexts.tsv\n",
      "Converted /Volumes/Users/ly_k1/Documents/mColBERT/data/processed/queries-test.jsonl to /Volumes/Users/ly_k1/Documents/mColBERT/data/testing/queries.tsv\n"
     ]
    }
   ],
   "source": [
    "import jsonlines\n",
    "\n",
    "def jsonl_to_tsv(data_filepath, output_data_filepath):\n",
    "    \"\"\"\n",
    "    Convert JSONL file with format {id:_,data:_} to TSV file with format id\\tdata\n",
    "    \"\"\"\n",
    "    output_data_file = open(output_data_filepath,\"w\")\n",
    "    with open(data_filepath, \"r\") as data_file:\n",
    "        reader = jsonlines.Reader(data_file)\n",
    "        for line in reader:\n",
    "            for key, value in line.items():\n",
    "                if key.endswith(\"_id\"):\n",
    "                    id = value\n",
    "                else:\n",
    "                    val = value.replace(\"\\n\",\"\")\n",
    "            if val and not val.isspace():\n",
    "                output_data_file.write(\"{id}\\t{data}\\n\".format(id = id, data = val))\n",
    "    output_data_file.close()\n",
    "    print(\"Converted {file} to {output_file}\".format(file = data_filepath, output_file = output_data_filepath))\n",
    "\n",
    "jsonl_to_tsv(os.path.join(PROCESSED_DATA_DIR,\"contexts-test.jsonl\"), os.path.join(TESTING_DATA_DIR,\"contexts.tsv\"))\n",
    "jsonl_to_tsv(os.path.join(PROCESSED_DATA_DIR,\"queries-test.jsonl\"), os.path.join(TESTING_DATA_DIR,\"queries.tsv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "sys.path.append(os.getenv('ROOT_DIR'))\n",
    "\n",
    "RAW_DATA_DIR = os.getenv('RAW_DATA_DIR')\n",
    "PROCESSED_DATA_DIR = os.getenv('PROCESSED_DATA_DIR')\n",
    "TESTING_DATA_DIR = os.getenv('TESTING_DATA_DIR')\n",
    "PROCESSED_TESTING_CONTEXTS_PATH = os.path.join(PROCESSED_DATA_DIR,\"contexts-test.jsonl\")\n",
    "PROCESSED_TESTING_QUERIES_PATH = os.path.join(PROCESSED_DATA_DIR,\"queries-test.jsonl\")\n",
    "DATA_LANG_DICT = {\"squad\":\"en\", \"korquad\":\"ko\", \"fquad\":\"fr\", \"germanquad\":\"de\", \"uitviquad\":\"vi\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "\n",
    "def get_ground_truth(data_filepath):\n",
    "    res = {}\n",
    "    with jsonlines.open(data_filepath) as reader:\n",
    "        for line in reader:\n",
    "            res[line[\"query_id\"]] = {\"context_id\":line[\"context_id\"],\"query_lang\":line[\"query_lang\"], \"context_lang\":line[\"context_lang\"]}\n",
    "    return res\n",
    "\n",
    "\n",
    "ground_truth = {}\n",
    "for data_name, data_lang in DATA_LANG_DICT.items():\n",
    "    data_filepath = os.path.join(PROCESSED_DATA_DIR, \"{data}-test-translated.jsonl\".format(data=data_name))\n",
    "    ground_truth.update(get_ground_truth(data_filepath))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Apr 22, 11:52:33] #> Loading the queries from /Volumes/Users/ly_k1/Documents/mColBERT/data/testing/queries.tsv ...\n",
      "[Apr 22, 11:52:33] #> Got 121945 queries. All QIDs are unique.\n",
      "\n",
      "[Apr 22, 11:52:33] #> Loading collection...\n",
      "0M \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Loaded 121,945 queries and 4,830 passages'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import colbert\n",
    "from colbert import Indexer, Searcher\n",
    "from colbert.infra import Run, RunConfig, ColBERTConfig\n",
    "from colbert.data import Queries, Collection\n",
    "\n",
    "nbits = 2   # encode each dimension with 2 bits\n",
    "doc_maxlen = 300   # truncate passages at 300 tokens\n",
    "index_name = f'{nbits}bits'\n",
    "\n",
    "checkpoint = '/Volumes/Users/ly_k1/Documents/mColBERT/experiments/default/none/2024-04/13/23.13.02/checkpoints/colbert'\n",
    "collection = os.path.join(TESTING_DATA_DIR,\"contexts.tsv\")\n",
    "queries = os.path.join(TESTING_DATA_DIR,\"queries.tsv\")\n",
    "queries = Queries(path=queries)\n",
    "collection = Collection(path=collection)\n",
    "\n",
    "f'Loaded {len(queries):,} queries and {len(collection):,} passages'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#index\n",
    "with Run().context(RunConfig(nranks=1, experiment='testing-official')):  # nranks specifies the number of GPUs to use.\n",
    "    config = ColBERTConfig(doc_maxlen=doc_maxlen, nbits=nbits)\n",
    "\n",
    "    indexer = Indexer(checkpoint=checkpoint, config=config)\n",
    "    indexer.index(name=index_name, collection=collection, overwrite=True)\n",
    "indexer.get_index() # You can get the absolute path of the index, if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Apr 22, 11:53:16] #> Loading collection...\n",
      "0M \n",
      "[Apr 22, 11:53:17] #> Loading codec...\n",
      "[Apr 22, 11:53:17] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Apr 22, 11:53:17] Loading packbits_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Apr 22, 11:53:18] #> Loading IVF...\n",
      "[Apr 22, 11:53:18] #> Loading doclens...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 844.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Apr 22, 11:53:18] #> Loading codes and residuals...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 63.91it/s]\n"
     ]
    }
   ],
   "source": [
    "with Run().context(RunConfig(experiment='testing-official')):\n",
    "    searcher = Searcher(index=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "121945it [05:51, 347.29it/s]\n"
     ]
    }
   ],
   "source": [
    "res = searcher.search_all(queries, k=10).todict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_dict = {k1:{k2:0 for k2 in [\"en\", \"de\", \"ko\", \"fr\", \"vi\"]} for k1 in [\"en\", \"de\", \"ko\", \"fr\", \"vi\"]}\n",
    "\n",
    "for qid, row in ground_truth.items():\n",
    "    sum_dict[row[\"query_lang\"]][row[\"context_lang\"]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': {'en': 0.8887417218543047,\n",
       "  'de': 0.7735934664246824,\n",
       "  'ko': 0.8089712504329754,\n",
       "  'fr': 0.7782308657465495,\n",
       "  'vi': 0.8518658122879759},\n",
       " 'de': {'en': 0.8573320719016083,\n",
       "  'de': 0.8008166969147006,\n",
       "  'ko': 0.6629719431936266,\n",
       "  'fr': 0.7474905897114178,\n",
       "  'vi': 0.7719562759140596},\n",
       " 'ko': {'en': 0.8142857142857143,\n",
       "  'de': 0.7100725952813067,\n",
       "  'ko': 0.9468306200207828,\n",
       "  'fr': 0.6831869510664994,\n",
       "  'vi': 0.7463249151903506},\n",
       " 'fr': {'en': 0.8573320719016083,\n",
       "  'de': 0.7490925589836661,\n",
       "  'ko': 0.6705923103567717,\n",
       "  'fr': 0.8296737766624843,\n",
       "  'vi': 0.7779871843196381},\n",
       " 'vi': {'en': 0.8340586565752128,\n",
       "  'de': 0.7218693284936479,\n",
       "  'ko': 0.7185659854520263,\n",
       "  'fr': 0.7082810539523212,\n",
       "  'vi': 0.9140595552205051}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top10_dict = {k1:{k2:0 for k2 in [\"en\", \"de\", \"ko\", \"fr\", \"vi\"]} for k1 in [\"en\", \"de\", \"ko\", \"fr\", \"vi\"]}\n",
    "\n",
    "res_dict = res\n",
    "\n",
    "for qid, rankings in res_dict.items():\n",
    "    row = ground_truth[qid]\n",
    "    pids = [tup[0] for tup in rankings]\n",
    "    if row[\"context_id\"] in pids:\n",
    "        top10_dict[row[\"query_lang\"]][row[\"context_lang\"]] += 1\n",
    "\n",
    "accuracy = {}\n",
    "for lang in sum_dict.keys():\n",
    "    accuracy[lang] = {}\n",
    "    for context_lang in sum_dict[lang].keys():\n",
    "        accuracy[lang][context_lang] = top10_dict[lang][context_lang] / sum_dict[lang][context_lang]\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': {'en': 0.867833491012299,\n",
       "  'de': 0.7617967332123412,\n",
       "  'ko': 0.7753723588500173,\n",
       "  'fr': 0.743099121706399,\n",
       "  'vi': 0.8198266113833396},\n",
       " 'de': {'en': 0.8319772942289498,\n",
       "  'de': 0.7921960072595281,\n",
       "  'ko': 0.6179425008659508,\n",
       "  'fr': 0.705771643663739,\n",
       "  'vi': 0.7263475310968714},\n",
       " 'ko': {'en': 0.7771050141911069,\n",
       "  'de': 0.6928312159709619,\n",
       "  'ko': 0.9350536889504676,\n",
       "  'fr': 0.6358218318695107,\n",
       "  'vi': 0.7108933283075763},\n",
       " 'fr': {'en': 0.830558183538316,\n",
       "  'de': 0.7318511796733213,\n",
       "  'ko': 0.6271215794942847,\n",
       "  'fr': 0.8036386449184442,\n",
       "  'vi': 0.7327553712777988},\n",
       " 'vi': {'en': 0.8026490066225166,\n",
       "  'de': 0.707350272232305,\n",
       "  'ko': 0.6827156217526844,\n",
       "  'fr': 0.6590338770388958,\n",
       "  'vi': 0.8993592159819073}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top5_dict = {k1:{k2:0 for k2 in [\"en\", \"de\", \"ko\", \"fr\", \"vi\"]} for k1 in [\"en\", \"de\", \"ko\", \"fr\", \"vi\"]}\n",
    "\n",
    "res_dict = res\n",
    "\n",
    "for qid, rankings in res_dict.items():\n",
    "    row = ground_truth[qid]\n",
    "    pids = [tup[0] for tup in rankings[:5]]\n",
    "    if row[\"context_id\"] in pids:\n",
    "        top5_dict[row[\"query_lang\"]][row[\"context_lang\"]] += 1\n",
    "\n",
    "accuracy = {}\n",
    "for lang in sum_dict.keys():\n",
    "    accuracy[lang] = {}\n",
    "    for context_lang in sum_dict[lang].keys():\n",
    "        accuracy[lang][context_lang] = top5_dict[lang][context_lang] / sum_dict[lang][context_lang]\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': {'en': 0.7487228003784295,\n",
       "  'de': 0.6746823956442831,\n",
       "  'ko': 0.6222722549359196,\n",
       "  'fr': 0.5699498117942283,\n",
       "  'vi': 0.6532227666792311},\n",
       " 'de': {'en': 0.6923368022705771,\n",
       "  'de': 0.7245916515426497,\n",
       "  'ko': 0.4620713543470731,\n",
       "  'fr': 0.541405269761606,\n",
       "  'vi': 0.5454202789295137},\n",
       " 'ko': {'en': 0.6161778618732261,\n",
       "  'de': 0.6048094373865699,\n",
       "  'ko': 0.8481122272254936,\n",
       "  'fr': 0.4598494353826851,\n",
       "  'vi': 0.5367508480964945},\n",
       " 'fr': {'en': 0.689120151371807,\n",
       "  'de': 0.6424682395644283,\n",
       "  'ko': 0.4624177346726706,\n",
       "  'fr': 0.6609159347553325,\n",
       "  'vi': 0.5702977761025254},\n",
       " 'vi': {'en': 0.6463576158940397,\n",
       "  'de': 0.6188747731397459,\n",
       "  'ko': 0.5261517145826117,\n",
       "  'fr': 0.4883939774153074,\n",
       "  'vi': 0.7810026385224275}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top1_dict = {k1:{k2:0 for k2 in [\"en\", \"de\", \"ko\", \"fr\", \"vi\"]} for k1 in [\"en\", \"de\", \"ko\", \"fr\", \"vi\"]}\n",
    "\n",
    "res_dict = res\n",
    "\n",
    "for qid, rankings in res_dict.items():\n",
    "    row = ground_truth[qid]\n",
    "    pids = [tup[0] for tup in rankings[:1]]\n",
    "    if row[\"context_id\"] in pids:\n",
    "        top1_dict[row[\"query_lang\"]][row[\"context_lang\"]] += 1\n",
    "\n",
    "accuracy = {}\n",
    "for lang in sum_dict.keys():\n",
    "    accuracy[lang] = {}\n",
    "    for context_lang in sum_dict[lang].keys():\n",
    "        accuracy[lang][context_lang] = top1_dict[lang][context_lang] / sum_dict[lang][context_lang]\n",
    "accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
